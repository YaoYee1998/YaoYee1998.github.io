<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Yaoyee"><link rel="alternative" href="/atom.xml" title="I'm Yaoyee" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title> - I'm Yaoyee</title><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--><script src="/js/jquery-3.1.1.min.js"></script><script src="/js/fancybox/jquery.fancybox.min.js"></script></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">I'm Yaoyee</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">カタログ/（目录）</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"> <time class="post__time" datetime="2018-12-26T16:40:58.830Z">December 27, 2018</time><h1 class="post__title"><a href="/2018/12/27/大数据处理平台Hadoop之安装/"></a></h1><div class="post__main echo"><p>–</p>
<h1 id="title：大数据处理平台Hadoop之安装"><a href="#title：大数据处理平台Hadoop之安装" class="headerlink" title="title：大数据处理平台Hadoop之安装"></a>title：大数据处理平台Hadoop之安装</h1><p>–</p>
<h1 id="基于ubuntu的Hadoop2-9-0的安装步骤-Hadoop2-X-X同适用"><a href="#基于ubuntu的Hadoop2-9-0的安装步骤-Hadoop2-X-X同适用" class="headerlink" title="基于ubuntu的Hadoop2.9.0的安装步骤(Hadoop2.X.X同适用)"></a>基于ubuntu的Hadoop2.9.0的安装步骤(Hadoop2.X.X同适用)</h1><h2 id="1-Linux-系统-，可在虚拟机安装"><a href="#1-Linux-系统-，可在虚拟机安装" class="headerlink" title="1.Linux 系统 ，可在虚拟机安装"></a>1.Linux 系统 ，可在虚拟机安装</h2><h2 id="2-创建Hadoop用户"><a href="#2-创建Hadoop用户" class="headerlink" title="2.创建Hadoop用户"></a>2.创建Hadoop用户</h2><p>若安装Linux系统时创建的用户不是hadoop，则需要创建一个名为hadoop的用户，<br>步骤如下：<br>（1） 使用root进入linux系统<br>（2） 打开linux命令行终端（没装图形化界面的忽略本步骤）<br>（3） 使用语句创建一个名为hadoop的用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd –m hadoop –s /bin/bash1</span><br></pre></td></tr></table></figure>
<p>（4） 修改密码<br><img src="https://img-blog.csdn.net/20171126222644707?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20171126222712119?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>（5） 为hadoop用户增加管理员权限，方便以后的操作<br>在命令行输入visudo命令出现如下界面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo adduser hadoop sudo1</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20171126222731695?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="3-使用hadoop用户登陆linux系统"><a href="#3-使用hadoop用户登陆linux系统" class="headerlink" title="3.使用hadoop用户登陆linux系统"></a>3.使用hadoop用户登陆linux系统</h2><h2 id="4-更新apt"><a href="#4-更新apt" class="headerlink" title="4.更新apt"></a>4.更新apt</h2><p>后面会用apt安装一些软件，如果没更新，可能有一些会出错，使用以下语句更新</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update1</span><br></pre></td></tr></table></figure>
<h2 id="5-安装vim"><a href="#5-安装vim" class="headerlink" title="5.安装vim"></a>5.安装vim</h2><p>后边会修改配置文件，使用vim编辑器用法和vi相同，很好用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vim      #安装vim1</span><br></pre></td></tr></table></figure>
<p>安装有提示 按提示输入y即可</p>
<h2 id="6-安装配置SSH"><a href="#6-安装配置SSH" class="headerlink" title="6.安装配置SSH"></a>6.安装配置SSH</h2><p>SSH 为 Secure Shell 的缩写，是建立在应用层和传输层基础上的安全协议。SSH 是目前较可靠、专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。SSH最初是UNIX系统上的一个程序，后来又迅速扩展到其他操作平台。 SSH是由客户端和服务端的软件组成，服务端是一个守护进程(daemon)，它在后台运行并响应来自客户端的连接请求，客户端包含ssh程序以及像scp（远程拷贝）、slogin（远程登陆）、sftp（安全文件传输）等其他的应用程序<br>Hadoop名称节点（NameNode）需要启动集群中所有机器的Hadoop守护进程，这个过程需要通过SSH登录来实现。Hadoop并没有提供SSH输入密码登录的形式，因此，为了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们</p>
<h3 id="（1）-执行以下命令进行校验是否装有SSH-Clint-和-SSH-server（red-hat和cente一般自带的都有）"><a href="#（1）-执行以下命令进行校验是否装有SSH-Clint-和-SSH-server（red-hat和cente一般自带的都有）" class="headerlink" title="（1） 执行以下命令进行校验是否装有SSH Clint 和 SSH server（red hat和cente一般自带的都有）"></a>（1） 执行以下命令进行校验是否装有SSH Clint 和 SSH server（red hat和cente一般自带的都有）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm –qa | grep ssh1</span><br></pre></td></tr></table></figure>
<p>若有则出现其版本号，如下图，则不需要再安装<br><img src="https://img-blog.csdn.net/20171126222830269?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="（2）-若需要安装，那就安装…（ubuntu默认安装了SSH-client-需要手动安装SSH-server）"><a href="#（2）-若需要安装，那就安装…（ubuntu默认安装了SSH-client-需要手动安装SSH-server）" class="headerlink" title="（2） 若需要安装，那就安装…（ubuntu默认安装了SSH client 需要手动安装SSH server）"></a>（2） 若需要安装，那就安装…（ubuntu默认安装了SSH client 需要手动安装SSH server）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server     #安装SSH server1</span><br></pre></td></tr></table></figure>
<h3 id="（3）-之后通过以下命令验证"><a href="#（3）-之后通过以下命令验证" class="headerlink" title="（3） 之后通过以下命令验证"></a>（3） 之后通过以下命令验证</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost1</span><br></pre></td></tr></table></figure>
<p>按提示先输入yes，再输入hadoop用户的密码，如下图，之后就登陆上了<br><img src="https://img-blog.csdn.net/20171126222909969?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="（4）-因为每次登陆ssh是都需要输入密码比较麻烦，所以使用以下语句配置一下无密码登录"><a href="#（4）-因为每次登陆ssh是都需要输入密码比较麻烦，所以使用以下语句配置一下无密码登录" class="headerlink" title="（4） 因为每次登陆ssh是都需要输入密码比较麻烦，所以使用以下语句配置一下无密码登录"></a>（4） 因为每次登陆ssh是都需要输入密码比较麻烦，所以使用以下语句配置一下无密码登录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">exit                           # 退出刚才的 ssh localhost</span><br><span class="line">cd ~/.ssh/                     # 若没有该目录，请先执行一次ssh localhost</span><br><span class="line">ssh-keygen -t rsa              # 会有提示，都按回车就可以</span><br><span class="line">cat ./id_rsa.pub &gt;&gt; ./authorized_keys  # 加入授权1234</span><br></pre></td></tr></table></figure>
<h3 id="（5）-之后再用ssh-localhost登陆，无需输入密码就成功了"><a href="#（5）-之后再用ssh-localhost登陆，无需输入密码就成功了" class="headerlink" title="（5） 之后再用ssh localhost登陆，无需输入密码就成功了"></a>（5） 之后再用ssh localhost登陆，无需输入密码就成功了</h3><p><img src="https://img-blog.csdn.net/20171126222930801?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="7-安装java环境"><a href="#7-安装java环境" class="headerlink" title="7.安装java环境"></a>7.安装java环境</h2><p>使用以下命令安装openjdk，但注意需要联网</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install default-jre default-jdk1</span><br></pre></td></tr></table></figure>
<p>之后通过vim编辑器写给环境变量，将javahome加入其中，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc1</span><br></pre></td></tr></table></figure>
<p>在文件最前面一行加入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/default-java1</span><br></pre></td></tr></table></figure>
<p>之后按键盘esc退出编辑模式，再输入命令 :wq保存并退出vim<br>接下来让环境变量立即生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc    # 使变量设置生效1</span><br></pre></td></tr></table></figure>
<p>最后可以检验一下是否设置正确</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo $JAVA_HOME     # 检验变量值</span><br><span class="line">java -version</span><br><span class="line">$JAVA_HOME/bin/java -version  # 与直接执行java -version一样123</span><br></pre></td></tr></table></figure>
<p>Java环境安装结束</p>
<h2 id="8-安装Hadoop2"><a href="#8-安装Hadoop2" class="headerlink" title="8.安装Hadoop2"></a>8.安装Hadoop2</h2><h3 id="（1）可以通过https-dist-apache-org-repos-dist-release-hadoop-common"><a href="#（1）可以通过https-dist-apache-org-repos-dist-release-hadoop-common" class="headerlink" title="（1）可以通过https://dist.apache.org/repos/dist/release/hadoop/common/"></a>（1）可以通过<a href="https://dist.apache.org/repos/dist/release/hadoop/common/" target="_blank" rel="noopener">https://dist.apache.org/repos/dist/release/hadoop/common/</a></h3><p>下载（在虚拟机的浏览器中打开教程点击此链接下载），一般选择下载最新的稳定版本，即下载 “stable” 下的 hadoop-2.x.y.tar.gz 这个格式的文件，这是编译好的，另一个包含 src 的则是 Hadoop 源代码，需要进行编译才可使用。<br>下载后一般可以直接使用，如果网络不好可能导致文件不完整，可以在下载后先进性校验：<br>校验方法：<br>下载官方网站提供的 hadoop-2.x.y.tar.gz.mds 这个文件（<a href="https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-2.9.0/" target="_blank" rel="noopener">https://dist.apache.org/repos/dist/release/hadoop/common/hadoop-2.9.0/</a>可能会出现点击那个文件直接在网页上打开的情况（无法下载），此时只需执行下面第二条指令，出来一串16禁止字符 和网页上的以一比较就行），该文件包含了检验值可用于检查 hadoop-2.x.y.tar.gz 的完整性，本文涉及的文件均通过浏览器下载，默认保存在 “下载” 目录中（若不是请自行更改 tar 命令的相应目录）。另外，版本号不对应自行修改成你下载的版本号。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat ~/下载/hadoop-2.9.0.tar.gz.mds | grep &apos;MD5&apos; # 列出md5检验值</span><br><span class="line">md5sum ~/下载/hadoop-2.9.0.tar.gz | tr &quot;a-z&quot; &quot;A-Z&quot; # 计算md5值，并转化为大写，方便比较12</span><br></pre></td></tr></table></figure>
<p>运行之后显示如下 比较两字符串，若相同则正确，不同则文件损坏，一定要重新下载！！<br><img src="https://img-blog.csdn.net/20171126223009556?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="（2）将下载的文件解压安装"><a href="#（2）将下载的文件解压安装" class="headerlink" title="（2）将下载的文件解压安装"></a>（2）将下载的文件解压安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf ~/下载/hadoop-2.9.0.tar.gz -C /usr/local    # 解压到/usr/local中</span><br><span class="line">cd /usr/local/</span><br><span class="line">sudo mv ./hadoop-2.9.0/ ./hadoop            # 将文件夹名改为hadoop</span><br><span class="line">sudo chown -R hadoop ./hadoop       # 修改文件权限1234</span><br></pre></td></tr></table></figure>
<h3 id="（3）使用以下命令检查hadoop是否可用"><a href="#（3）使用以下命令检查hadoop是否可用" class="headerlink" title="（3）使用以下命令检查hadoop是否可用"></a>（3）使用以下命令检查hadoop是否可用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hadoop version12</span><br></pre></td></tr></table></figure>
<h3 id="（4）伪分布式配置"><a href="#（4）伪分布式配置" class="headerlink" title="（4）伪分布式配置"></a>（4）伪分布式配置</h3><p>Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。<br>Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。<br>首先修改core-site.xml，使用命令<code>gedit ./etc/hadoop/core-site.xml</code>打开将其中的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;/configuration&gt;12</span><br></pre></td></tr></table></figure>
<p>修改为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;1234567891011</span><br></pre></td></tr></table></figure>
<p>之后修改配置文件hdfs-site.xml中相同位置的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;1234567891011121314</span><br></pre></td></tr></table></figure>
<p>配置文件修改说明：<br>Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。<br>此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。</p>
<p>配置完成后，执行以下语句格式化NameNode：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs namenode –format1</span><br></pre></td></tr></table></figure>
<p>成功会看到如下提示：<br><img src="https://img-blog.csdn.net/20171126223038728?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>接着执行以下命令开启 NameNode 和 DataNode 守护进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-dfs.sh  #start-dfs.sh是个完整的可执行文件，中间没有空格1</span><br></pre></td></tr></table></figure>
<p>若ssh提示连接，输入yes即可<br><img src="https://img-blog.csdn.net/20171126223053265?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>启动完成后，输入命令jps来判断是否启动成功<br><img src="https://img-blog.csdn.net/20171126223105389?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>若成功启动则会列出如下进程: “NameNode”、”DataNode” 和 “SecondaryNameNode”</p>
<p>成功启动后，可以访问 Web 界面 <a href="http://localhost:50070/" target="_blank" rel="noopener">http://localhost:50070</a> 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。<br>若有如下页面，恭喜安装成功。<br><img src="https://img-blog.csdn.net/20171126223124496?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRWR3aW5CYWxhbmNl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="补充："><a href="#补充：" class="headerlink" title="补充："></a>补充：</h3><p>每次打开hadoop时总切换到hadoop目录十分麻烦，可以将开启hadoop启动程序的可执行文集那所在的目录加入到PATH变量中，以后启动直接输入程序名就可以打开了十分方便。<br>使用命令：<code>vim ~/.bashrc</code><br>打开后，在文件最前边加入以下语句。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/hadoop/sbin:/usr/local/hadoop/bin1</span><br></pre></td></tr></table></figure>
<p>之后保存退出，使用语句<code>source ~/.bashrc</code>使配置生效。</p>
</div></header></article><div class="comments" id="lv-container" data-id="city" data-uid="your uid"><script>(function(d, s) {var j, e = d.getElementsByTagName(s)[0];if (typeof LivereTower === 'function') { return; } j = d.createElement(s);j.src = 'https://cdn-city.livere.com/js/embed.dist.js';j.async = true;e.parentNode.insertBefore(j, e);})(document, 'script');</script></div></main><footer class="foot"><div class="foot-copy">&copy; 2016-2018 Yaoyee</div></footer><script src="/js/scroller.js"></script><script src="/js/main.js"></script></body></html>